{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44cd8c37-d328-43be-894a-4d6fd33eb821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, I must not be the only one who attepted to replicate some medium or towards data science\n",
    "# we should probably rename it or create a new one toward AGI guides on LLM and become \n",
    "# incessantly frustrated about how to properly setup all the libary versions and drivers.\n",
    "# Or maybe not, probably not because it is not that complicated. Alas, besides going over some startup LLM\n",
    "# pondering some idea projects and applications such as what Mistral actualyl good for, i'll also include some\n",
    "# sample code that automatically captures your python transformers, langchain, openai lbirary versions and saves them for\n",
    "# later use. If you need it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7950c5f5-a6e4-4f9c-be76-448cc5da0b1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !conda install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import pytorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# !conda install bitsandbytes accelerate xformers einops langchain faiss-cpu transformers sentence-transformers\n",
    "import torch\n",
    "# import pytorch\n",
    "torch.zeros(1).cuda()\n",
    "\n",
    "# The magic line to fix torch due to cuda version compatibility issues\n",
    "  # 378  conda install pytorch=*=*cuda* cudatoolkit -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3da6204-0b2c-47e7-8528-6dec10a9a1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5241d795-340d-448e-be77-66078e094f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "NVIDIA GeForce RTX 4080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, BitsAndBytesConfig\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# >>> Device: cuda\n",
    "# >>> Tesla T4\n",
    "import pkg_resources, sys, os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec30b7-e9e5-4539-9295-46f0d91975fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d6c53b-1aed-4763-a2e4-d86a14e880f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistral_7B_mlm\n"
     ]
    }
   ],
   "source": [
    "# echo and get the current conda env name\n",
    "!echo $CONDA_DEFAULT_ENV\n",
    "\n",
    "conda_env_name = os.getenv('CONDA_DEFAULT_ENV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1851c920-86e1-4f46-85ba-9a858c8d09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a CUDA device availability check\n",
    "def get_cuda_device():\n",
    "    if torch.cuda.is_available:\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "def get_cuda_device():\n",
    "    if torch.cuda.is_available:\n",
    "        return \"cuda:0\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d09d31-1729-46c8-ab73-1905b8082e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9733ac82ae45f5870d1106e49029e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "orig_model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_path = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\" # quantized model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                # load_in_8bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(orig_model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "# text = \"What is Mistral? Write a short answer but apt answer that would pass a college test.\"\n",
    "# mistral_llm.invoke(text)\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    temperature=1.0,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=378,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e747114-7b7b-4fa2-b1a6-9260a7d0e3cc",
   "metadata": {},
   "source": [
    "### Creating the LLM Pipeline with what appears to be Huggingface pipeline\n",
    "- transformers.pipeline\n",
    "- Now let's create the LLM pipeline. It is very simple to do using the transformers pipeline capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d04c3-3d1a-4d88-9aed-af813a217dad",
   "metadata": {},
   "source": [
    "#### Using Langchain framework with LLMs\n",
    "- LangChain is a cross platform python framework that supports diferent language models without the ai engineer having to write additional support code.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1982d517-55bb-4d20-b31b-60ffc7bcde1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is the usual composition of rocket fuel? [/INST] The usual composition of rocket fuel varies depending on the specific type of rocket and its intended purpose. However, common ingredients in rocket fuel include nitrogen oxides (such as nitrogen peroxide or monomethyl hydrazine), alcohols (such as ethylene oxide or isopropyl alcohol), ketones (such as acetone or benzene), and other chemicals that produce hot gases when burned. These chemicals are chosen for their ability to produce high levels of thrust, efficient combustion, and low toxicity. The precise mixture of fuels used in a rocket can be optimized to achieve the desired performance characteristics.</s>\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "messages = [\n",
    "        {\"role\": \"user\", \"content\": \"What is the usual composition of rocket fuel?\"}\n",
    "]\n",
    "\n",
    "chat_encodes = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = chat_encodes.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=256, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24a542c6-dcfe-4032-8ae1-1894245768cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def get_llm(text_prompt):\n",
    "    return mistral_llm.invoke(text_prompt)\n",
    "\n",
    "# Define the column width for text wrapping (Medium's typical width is around 80 characters)\n",
    "COLUMN_WIDTH = 76\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove newline characters and any other special characters you want to strip\n",
    "    cleaned_text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def wrap_text(text, width):\n",
    "    # Use textwrap to wrap text to the specified width\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    return wrapped_text\n",
    "\n",
    "def wrap_text_with_comments(text, width):\n",
    "    # Wrap the text to the specified width\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    # Split the wrapped text into lines\n",
    "    lines = wrapped_text.split('\\n')\n",
    "    # Add a comment character at the start of each line\n",
    "    commented_lines = ['# ' + line for line in lines]\n",
    "    # Join the lines back together\n",
    "    commented_text = '\\n'.join(commented_lines)\n",
    "    return commented_text\n",
    "\n",
    "\n",
    "def get_llm_response(text_prompt):\n",
    "    # Get the response from the model\n",
    "    response = mistral_llm.invoke(text_prompt)\n",
    "    # Clean the output text\n",
    "    cleaned_response = clean_text(response)\n",
    "    # Wrap the cleaned response text to the specified column width\n",
    "    wrapped_response = wrap_text_with_comments(cleaned_response, COLUMN_WIDTH)\n",
    "    return wrapped_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cf7476-b5fe-4f6a-a70d-e47da5365263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Mistral? Write a short answer but apt answer that would pass a college test.\\n\\nMistral is a type of wind that blows from the Mediterranean Sea towards the French Alps. It is known for its strong and cold winds, which can reach speeds of up to 100 km/h (62 mph). The mistral is a common weather phenomenon in the region and is often associated with extreme conditions, such as heavy snowfall and freezing temperatures.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is the usual composition of rocket fuel?\"\n",
    "mistral_llm.invoke(text)\n",
    "text = \"What is Mistral? Write a short answer but apt answer that would pass a college test.\"\n",
    "mistral_llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b316c7ce-75fb-41c7-a95a-d044d168e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Mistral? Write a short answer but apt answer that would pass a college test.\n",
      "\n",
      "Mistral is a type of wind that blows from the Mediterranean Sea towards the French Alps. It is known for its strong and cold winds, which can reach speeds of up to 100 km/h (62 mph). The mistral is a common weather phenomenon in the region and is often associated with extreme conditions, such as heavy snowfall and freezing temperatures.\n"
     ]
    }
   ],
   "source": [
    "mistral_out = mistral_llm.invoke(text)\n",
    "print(mistral_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b61e8d34-796b-4037-98f5-e19a2c269c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What is Mistral? Write a short answer but apt answer that would pass a\n",
      "# college test.  Mistral is a type of wind that blows from the Mediterranean\n",
      "# Sea towards the French Alps. It is known for its strong and cold winds,\n",
      "# which can reach speeds of up to 100 km/h (62 mph). The mistral is a common\n",
      "# weather phenomenon in the region and is often associated with extreme\n",
      "# conditions, such as heavy snowfall and freezing temperatures.\n"
     ]
    }
   ],
   "source": [
    "clean_response = get_llm_response(text)\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ed8a1a-14d9-48ea-8b56-aadc79b96ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_test = \"What is Mistral AI? Please write a cohesive, apt answer that would be deemed college exam worthy.\"\n",
    "# mistral_llm.invoke(quick_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7119e87c-0af8-4631-82ae-314a319e335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# What is Mistral AI? Please write a cohesive, apt answer that would be deemed\n",
      "# college exam worthy.  Mistral AI is an innovative company based in Paris,\n",
      "# France, developing large language models. These models are capable of\n",
      "# generating human-like text and can be fine-tuned for specific tasks such as\n",
      "# translation, summarization, and question answering. The company's flagship\n",
      "# product, Mistral, is a powerful language model that has been trained on vast\n",
      "# amounts of data from the internet and can generate high-quality text in over\n",
      "# 100 languages.  One of the key features of Mistral AI is its ability to\n",
      "# generate text that is both fluent and contextually relevant. This is\n",
      "# achieved through the use of advanced natural language processing techniques\n",
      "# and machine learning algorithms that allow the model to understand the\n",
      "# meaning and context of words and phrases. Additionally, Mistral AI's models\n",
      "# are designed to be highly customizable, allowing users to fine-tune them for\n",
      "# specific applications and domains.  Overall, Mistral AI represents a\n",
      "# significant advancement in the field of natural language processing and has\n",
      "# the potential to revolutionize the way we interact with technology. By\n",
      "# enabling machines to generate high-quality text in a wide range of\n",
      "# languages, Mistral AI is helping to bridge the gap between humans and\n",
      "# computers and making it easier for people to communicate and collaborate\n",
      "# across language barriers.\n"
     ]
    }
   ],
   "source": [
    "quick_test = \"What is Mistral AI? Please write a cohesive, apt answer that would be deemed college exam worthy.\"\n",
    "# mistral_llm.invoke(quick_test)\n",
    "print(get_llm_response(quick_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed55d8-0205-46c9-91dc-fe8220feda59",
   "metadata": {},
   "source": [
    "#### Prompt input model using Transformers pipes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c41cbe4-6031-4980-9eff-7d39dcd71d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an AI assistant specialized in machine learning and rocketry.\n",
      "Answer the following question:\n",
      "Question: What starting mass does a rocket need to have to reach the Moons orbit?\n",
      "Answer:\n",
      "To reach the Moon's orbit, a rocket needs to have a certain amount of mass, which is dependent on various factors such as the mass of the rocket's payload, fuel capacity, and the rocket's thrust. However, it is worth noting that the Moon's orbit is approximately 238,855 miles (384,400 kilometers) away from Earth, and reaching that distance requires a significant amount of energy.\n",
      "Assuming that the rocket is unloaded and only has the necessary fuel to reach the Moon's orbit, the starting mass of the rocket would depend on the amount and type of fuel used. For example, if the rocket uses solid rocket fuel, its starting mass would be relatively low since the fuel is already in its combusted form. On the other hand, if the rocket uses liquid rocket fuel, its starting mass would be higher since the fuel must be stored in its liquid form.\n",
      "Therefore, the starting mass of a rocket that needs to reach the Moon's orbit would depend on the specific rocket design and its fuel capacity. Generally, it would require a significant amount of mass to overcome the gravitational pull of the Earth\n"
     ]
    }
   ],
   "source": [
    "# Creating a prompt type input model using only the Transformers pipeline library\n",
    "from transformers import pipeline\n",
    "\n",
    "def fill_prompt_template(template, **kwargs):\n",
    "    return template.format(**kwargs)\n",
    "\n",
    "# Define your actual prompt here\n",
    "test_template = \"\"\"\n",
    "You are an AI assistant specialized in machine learning and rocketry. Answer the following question:\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# defining the base prompt template with variable intro\n",
    "base_template = \"\"\"\n",
    "{intro}\n",
    "Answer the following question:\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate text using the custom prompt\n",
    "def generate_with_template(intro, question, max_new_tokens=256):\n",
    "    # Fill the new template with the intro and specific question\n",
    "    prompt = fill_prompt_template(base_template, intro=intro, question=question)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    device = get_cuda_device()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate the new text \n",
    "    generated_ids = model.generate(inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "\n",
    "    # Decode the generated text using tokenizer\n",
    "    decoded =  tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return decoded[0]\n",
    "\n",
    "# generated_ids = model.generate(model_inputs, max_new_tokens=256, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])\n",
    "\n",
    "# Do an example\n",
    "intro = \"You are an AI assistant specialized in machine learning and rocketry.\"\n",
    "question = \"What starting mass does a rocket need to have to reach the Moons orbit?\"\n",
    "response = generate_with_template(intro, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad5806a-8774-4e22-a633-38cd810f2b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What about 6 * 5 + 5, what is the asnwer?\n",
      "A: 30\n"
     ]
    }
   ],
   "source": [
    "text_four = \"What about 6 * 5 + 5, what is the asnwer?\"\n",
    "# mistral_llm.invoke(text_four)\n",
    "print(get_llm(text_four))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc707c22-85f8-4510-ab42-d54d16568a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the result of '5 plus 6 times 5'? And think about this, because the problem is two step one; 6 * 5 and then add another 5. What is the answer to that?\\nAnswer: 30\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qfive = \"What is the result of '5 plus 6 times 5'? And think about this, because the problem is two step one; 6 * 5 and then add another 5. What is the answer\"\n",
    "mistral_llm.invoke(qfive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a25c8-1530-4511-862d-249f3b215d86",
   "metadata": {},
   "source": [
    "LangChain - Python framework used for language models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f841d0-3b43-442c-ad4a-04975045f616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tell me a funny about chickens.\\n\\nI'm not much of a joke teller, but I'll give it a try. Here goes: Why don't chickens like to tell secrets? Because they might cluck up!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "        \"Tell me a {adjective} about {content}.\"\n",
    ")\n",
    "prompt.format(adjective=\"funny\", content=\"chickens\")\n",
    "# adjective in terms of format for prompt. If I wanted to use fact and or scientific fact I'd have to look it up\n",
    "\n",
    "llm_chain = prompt | mistral_llm\n",
    "llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddd3fa-1e30-419b-acc8-d0b8e0a30ec2",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory class\n",
    "- Behaves akin to ChatMessageHistory (for precending applications) such that it saves earlier messages.\n",
    "- Type of memory variable that allows for storing of earlier messages and extracting them in a variable. It is useful for creating chat models that can remember the previous chat instance content and context of the conversation, using earlier message data to better predict and improve subsequent generated answers to new prompts.\n",
    "\n",
    "1. Custom Applications - Conversation Buffer in LangChain - "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f124b4d-c357-4177-95e0-6684b67fd9f6",
   "metadata": {},
   "source": [
    "The ConversationBufferMemory class behaves somewhat like the ChatMessageHistory class with respect to the message store, though it provides clever methods to retrieve old messages.\n",
    "For example, we can retrieve old messages, as a list of messages or as one big string depending on what we need. If we want to ask the LLM to make a summary of the past conversation it might be useful to have the past conversation as one big string. If we want to do a detailed analysis of the past instead, we can read one message at a time by extracting a list.\n",
    "\n",
    ". This is a type of memory that allows for storing messages and then extracting them in a variable. It is useful for creating chat models that can remember the previous context of the conversation. You can find more information about it in these articles:\n",
    "\n",
    "Conversation Buffer | ï¸ Langchain: This notebook shows how to use ConversationBufferMemory with a simple example. It also shows how to use it in a chain with an OpenAI model.\n",
    "langchain.memory.buffer.ConversationBufferMemory: This is the official documentation of the ConversationBufferMemory class. It explains the parameters, methods, and attributes of the class.\n",
    "Memory | The Key to Intelligent Conversations - Towards AI: This is a blog post that gives an overview of the different types of memory used by Langchain, a platform for building intelligent conversational agents. It explains how ConversationBufferMemory works together with other memory types to manage the flow of conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89822a3e-410e-4e94-b6c0-fb2c90bffa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Mistral? Write a short answer but apt answer that would pass a college test.\\n\\nMistral is a type of wind that blows from the Mediterranean Sea towards the French Alps. It is known for its strong and cold winds, which can reach speeds of up to 100 km/h (62 mph). The mistral is a common weather phenomenon in the region and is often associated with extreme conditions, such as heavy snowfall and freezing temperatures.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What is Mistral? Write a short answer but apt answer that would pass a college test.\"\n",
    "mistral_llm.invoke(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cec95c-bb5c-445e-8d8a-fd2e708ee83f",
   "metadata": {},
   "source": [
    "- Once again LangChain - the special python framework that supports utilization of various LLM models. A special\n",
    "- framework specifically designed for large language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09eda086-9b33-4727-8819-e8bfb85d116a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tell me a funny joke about bears.\\n\\nWhy don't bears play cards in the wild? Too many cheetahs.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt.format(adjective=\"funny\", content=\"bears\")\n",
    "\n",
    "llm_chain = prompt | mistral_llm\n",
    "llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3db8-c142-4ac8-8110-940263c64c7a",
   "metadata": {},
   "source": [
    "#### How to use Prompts in LangChain\n",
    "1.  A prompt is a set of instructions or input such as a basic search question that the end user provides to guide the model's response, helping it or setting up the context for the model, and point it in the right direction such that it generates relevant and coherent language-based output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ed9b232-d6ab-4bda-b09e-a68acbe511f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"adjective\": \"funny\",\n",
      "  \"content\": \"chickens\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"adjective\": \"funny\",\n",
      "  \"content\": \"chickens\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Tell me a funny joke about chickens.\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:HuggingFacePipeline] [1.54s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Tell me a funny joke about chickens.\\n\\nWhy don't chickens like to tell jokes? They might crack each other up and all their eggs will scramble!\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.54s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Tell me a funny joke about chickens.\\n\\nWhy don't chickens like to tell jokes? They might crack each other up and all their eggs will scramble!\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Tell me a funny joke about chickens.\\n\\nWhy don't chickens like to tell jokes? They might crack each other up and all their eggs will scramble!\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = prompt | mistral_llm\n",
    "llm_chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"}, \n",
    "                 config={'callbacks': [ConsoleCallbackHandler()]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d22bab44-cae7-485c-a75b-d725c7510a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm...can I use this to pass feedback around to other functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee77402-3794-478b-8a38-0aba750c951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI guidance bot. Your name is Mistral. Answer the short sentences.\\n\\nMistral: Of course, I'm here to assist you with any questions or concerns you may have. How can I help you today?\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# used to create flexible templated prompts for chat models\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", \"You are a helpful AI guidance bot. Your name is {name}. Answer the short sentences.\"),\n",
    "    ]\n",
    ")\n",
    "llm_chain = chat_prompt | mistral_llm\n",
    "llm_chain.invoke({\"name\": \"Mistral\", \"user_input\": \"What is your name?\"})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb80515-a2cc-4005-979f-9d1e71625cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Need the following code if I am to run RetrievalQA for multi chain example\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "\n",
    "db_docs = [\"During a Mars opposition, the distance between Earth and Mars can be as short as 54.6 million kilometers, significantly reducing the travel time and fuel required for a mission.\"]\n",
    "\n",
    "# Creating a separate embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")\n",
    "\n",
    "vector_db = FAISS.from_texts(db_docs, embeddings)\n",
    "retriever = VectorStoreRetriever(vectorstore=vector_db)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f50ac083-cdce-4693-819e-8ed6c4255db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Chaining and Snowballing\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "# from langchain_core.memory import ConversationBufferMemory \n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.retrievers import DenseRetriever\n",
    "\n",
    "# First define the templates for multiple chains\n",
    "prompt_temp1 = \"\"\"You are an AI expert in astrophysics. Provide detailed information about the solar system and orbital mechanics.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# prompt_temp2 = \"\"\"Based on the information provided about the solar system, argue for the feasibility of interplanetary travel and colonization.\n",
    "# Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "prompt_temp2 = \"\"\"You are an expert aerospace engineer. Using prior information, discuss rocket and spacecraft design for interplanetary flight.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_temp3 = \"\"\"Given the information about interplanetary travel, discuss the potential challenges and solutions for establishing a colony on Mars.\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create PromptTemplate instances from the defined templates\n",
    "prompt1 = PromptTemplate.from_template(prompt_temp1)\n",
    "prompt2 = PromptTemplate.from_template(prompt_temp2)\n",
    "prompt3 = PromptTemplate.from_template(prompt_temp3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5a51987-5117-4786-ac54-91799f656449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"You are an expert aerospace engineer. Using prior information, discuss rocket and spacecraft design for interplanetary flight.\\nQuestion: {'question': 'Tell me about the solar system.', 'history': ''}\\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create the Chains\n",
    "# You will now create two chains using the defined prompt templates and the language model.\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the model parser\n",
    "model_parser = mistral_llm | StrOutputParser()\n",
    "\n",
    "# Define the chains\n",
    "chain1 = prompt1 | model_parser\n",
    "chain2 = prompt2 | model_parser\n",
    "# Step 4: Combine and Execute the Chains Sequentially\n",
    "# Combine the chains and run them in sequence using LCEL.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Combine the chains with RunnablePassthrough to maintain context\n",
    "combined_chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"history\": RunnablePassthrough()}\n",
    "    | {\"chain1_output\": chain1, \"chain2_output\": chain2}\n",
    ")\n",
    "\n",
    "# Create a memory buffer for context\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "\n",
    "# Final sequential chain with context memory\n",
    "sequential_chain = (combined_chain | {\"final_output\": chain2} | prompt2)\n",
    "\n",
    "# Example usage\n",
    "response = sequential_chain.invoke({\"question\": \"Tell me about the solar system.\", \"history\": \"\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c725b35-0eb5-443c-ae36-7d8b13e3efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Given the information about interplanetary travel, discuss the potential challenges and solutions for establishing a colony on Mars.\\nQuestion: You are an expert aerospace engineer. Using prior information, discuss rocket and spacecraft design for interplanetary flight.\\nQuestion: You are an AI expert in astrophysics. Provide detailed information about the solar system and orbital mechanics.\\nQuestion: {'question': 'Tell me about the solar system.'}\\nAnswer: The solar system is composed of a star at its center, our Sun, and all the objects that orbit around it, including planets, dwarf planets, moons, asteroids, comets, and other smaller bodies. The Sun makes up almost all of the total mass of the solar system.\\nThe solar system is divided into four main regions: the inner planetary region, the asteroid belt, the outer planetary region, and the Oort Cloud. The inner planetary region includes Mercury, Venus, Earth, and Mars, while the outer planetary region includes Jupiter, Saturn, Uranus, and Neptune. The asteroid belt lies between these two regions and is home to many asteroids and small planets.\\nOrbital mechanics refers to the study of how objects move in space, particularly how they move around a central star like the Sun. Orbit is determined by the gravitational force between the object and the star, as well as the object's initial velocity and direction of motion. The shape of an object's orbit is determined by Kepler's first law of planetary motion, which states that the orbit of every planet is an ellipse with the Sun at one of the two foci.\\nPlanets in the solar system have different types of orbits, depending on their distance from the Sun and their speed of motion. Inner planets, like Mercury and Venus, have nearly circular orbits that are close to the Sun and move quickly. Outer planets, like Jupiter and Saturn, have much larger orbits that are more elliptical and move more slowly.\\nIn addition to planets, other objects in the solar system also follow orbital mechanics, such as moons, asteroids, and comets. For example, the Moon follows an ellipt\\nAnswer: The solar system is composed of a star at its center, our Sun, and all the objects that orbit around it, including planets, dwarf planets, moons, asteroids, comets, and other smaller bodies. The Sun makes up almost all of the total mass of the solar system.\\nThe solar system is divided into four main regions: the inner planetary region, the asteroid belt, the outer planetary region, and the Oort Cloud. The inner planetary region includes Mercury, Venus, Earth, and Mars, while the outer planetary region includes Jupiter, Saturn, Uranus, and Neptune. The asteroid belt lies between these two regions and is home to many asteroids and small planets.\\nOrbital mechanics refers to the study of how objects move in space, particularly how they move around a central star like the Sun. Orbit is determined by the gravitational force between the object and the star, as well as the object's initial velocity and direction of motion. The shape of an object's orbit is determined by Kepler's first law of planetary motion, which states that the orbit of every planet is an ellipse with the Sun at one of the two foci.\\nPlanets in the solar system have different types of orbits, depending on their distance from the Sun and their speed of motion. Inner planets, like Mercury and Venus, have nearly circular orbits that are close to the Sun and move quickly. Outer planets, like Jupiter and Saturn, have much larger orbits that are more elliptical and move more slowly.\\nIn addition to planets, other objects in the solar system also follow orbital mechanics, such as moons, asteroids, and comets. For example, the Moon follows an ellipt\\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Initialize the model parser using the Mistral LLM and StrOutputParser\n",
    "model_parser = mistral_llm | StrOutputParser()\n",
    "\n",
    "# Define the first chain, which uses the first prompt template and the model parser\n",
    "first_chain = (\n",
    "    {\"question\": RunnablePassthrough()} | prompt1 | {\"question\": model_parser}\n",
    ")\n",
    "\n",
    "# Define the second chain, which uses the second prompt template and the model parser\n",
    "chain2 = prompt2 | model_parser\n",
    "\n",
    "# Define the final chain, which combines the first chain and the second chain, feeding the output of the first chain as input to the second\n",
    "final_chain = (\n",
    "    first_chain | {\"question\": chain2} | prompt3\n",
    ")\n",
    "\n",
    "# Example usage of the final_chain with a specific question\n",
    "response = final_chain.invoke({\"question\": \"Tell me about the solar system.\"})\n",
    "# Print the response from the final chain\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25895f70-f4ba-422e-920c-dba5bca21b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text=\"Given the information about interplanetary travel, discuss the potential challenges and solutions for establishing a colony on Mars.\\nQuestion: You are an expert aerospace engineer. Using prior information, discuss rocket and spacecraft design for interplanetary flight.\\nQuestion: You are an AI expert in astrophysics. Provide detailed information about the solar system and orbital mechanics.\\nQuestion: {'question': 'What is the approximate necessary size of the rocket required in order to make it to Mars?'}\\nAnswer: The size of the rocket required to make it to Mars would depend on several factors such as the mass of the payload, the amount of fuel needed for propulsion, and the design of the rocket itself. However, based on current estimates, a rocket with a diameter of around 10 meters (33 feet) and a length of around 60 meters (200 feet) would be required to carry a payload of around 100 tons to Mars. This is just an estimate and the actual size of the rocket may vary depending on the specific requirements of the mission.\\nAnswer: The solar system is made up of eight planets, including Earth, which orbit around a central star called the Sun. These planets are divided into two main types: terrestrial planets, which include Mercury, Venus, Earth, and Mars, and gas giants, which include Jupiter, Saturn, Uranus, and Neptune. The inner four planets are close to the Sun and are small and rocky, while the outer four planets are large and gaseous. The solar system also includes smaller bodies such as dwarf planets, asteroids, comets, and moons.\\nAnswer: Orbital mechanics is the study of the motion of objects in space, including planets, satellites, and other celestial bodies. It involves the calculation of orbits, gravitational forces, and other factors that affect the movement of objects in space. In order to achieve interplanetary flight, a spacecraft must be designed to travel through space and navigate its way to another planet. This requires careful planning and precise calculations of the trajectory and timing of the spacecraft's launch, as well as the use of advanced propulsion systems and navigation technologies.\\nAnswer:\"\n"
     ]
    }
   ],
   "source": [
    "new_response = final_chain.invoke({\"question\": \"What is the approximate necessary size of the rocket required in order to make it to Mars?\"})\n",
    "print(new_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8c7cd0c-3f14-4097-ad3e-bbcccea703dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/nomad/miniconda3/envs/mistral_7B_mlm/lib/python3.9/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Information about the solar system.', 'history': 'Previous chats about planets.', 'query': 'What is the largest planet in the solar system?', 'result': \"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\\nDuring a Mars opposition, the distance between Earth and Mars can be as short as 54.6 million kilometers, significantly reducing the travel time and fuel required for a mission.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nChat history: \\nQuestion: What is the largest planet in the solar system?\\nWrite your answers short. Helpful Answer: Jupiter\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the template for the prompt\n",
    "template = \"\"\"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n",
    "{context}\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Chat history: {history}\n",
    "Question: {question}\n",
    "Write your answers short. Helpful Answer:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"history\", \"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Initialize the QA chain with memory\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=mistral_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": False,\n",
    "        \"prompt\": prompt,\n",
    "        \"memory\": ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "    }\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "context = \"Information about the solar system.\"\n",
    "history = \"Previous chats about planets.\"\n",
    "question = \"What is the largest planet in the solar system?\"\n",
    "# In calling qa, instaed of \"question\" use \"query\"\n",
    "response = qa({\"context\": context, \"history\": history, \"query\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fececa61-cab6-4a15-9689-d2ecffbc6660",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_template1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setup the individual prompts using promptTemplate\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m prompt_ONE \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39m\u001b[43mpr_template1\u001b[49m, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# prompt2 = PromptTemplate(template=pr_template2, input_variables=[\"question\", \"context\"])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# prompt3 = PromptTemplate(template=pr_template3, input_variables=[\"question\", \"context\"])\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the chain with ConversationBufferMemory\u001b[39;00m\n\u001b[1;32m     11\u001b[0m chain1 \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     12\u001b[0m     llm\u001b[38;5;241m=\u001b[39mmistral_llm,\n\u001b[1;32m     13\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_template1' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup the individual prompts using promptTemplate\n",
    "prompt_ONE = PromptTemplate(template=pr_template1, input_variables=[\"history\",\"context\",\"question\"])\n",
    "# prompt2 = PromptTemplate(template=pr_template2, input_variables=[\"question\", \"context\"])\n",
    "# prompt3 = PromptTemplate(template=pr_template3, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "# Define a blank retriever\n",
    "# retriever = SimpleRetriever() # Use the basic retriever class, as a place holder for now.\n",
    "\n",
    "\n",
    "# Initialize the chain with ConversationBufferMemory\n",
    "chain1 = RetrievalQA.from_chain_type(\n",
    "    llm=mistral_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": False,\n",
    "        \"prompt\": prompt_ONE,\n",
    "        \"memory\": ConversationBufferMemory(memory_key=\"history\", \n",
    "                                           input_key=\"question\")\n",
    "    }\n",
    ")\n",
    "\n",
    "# chain2 = RetrievalQA.from_chain_type(\n",
    "#     llm=mistral_llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever,\n",
    "#     chain_type_kwargs={\n",
    "#         \"verbose\": False,\n",
    "#         \"prompt\": prompt2,\n",
    "#         \"memory\": ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# chain3 = RetrievalQA.from_chain_type(\n",
    "#     llm=mistral_llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=retriever,\n",
    "#     chain_type_kwargs={\n",
    "#         \"verbose\": False,\n",
    "#         \"prompt\": prompt3,\n",
    "#         \"memory\": ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
    "#     }\n",
    "# )\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
